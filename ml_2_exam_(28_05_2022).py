# -*- coding: utf-8 -*-
"""ML 2 Exam (28/05/2022).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tkPkmlMcwbww7E87-aW34gVVp1AH8xe3

#Part I: Unsupervised Learning
"""

# Import drive

from google.colab import drive
drive.mount('/content/drive')

# importing the dataset

import pandas as pd

df=pd.read_csv('/content/drive/MyDrive/Imarticus Datasets/Paper2/credit_card.csv')

"""Q1. The primary analysis of several categorical
features.
"""

df.head()

df.info()

df.shape

df.columns

df.describe()

df

# histogram

import matplotlib.pyplot as plt

fig=df.hist(figsize=(15,15),color='red')
plt.show()

# pair plot

import seaborn as sns

sns.pairplot(df)

# Heatmap

sns.heatmap(df.select_dtypes(['float64','int64']).corr(),annot=True)
plt.show()

"""Q2. Perform the following Exploratory Data Analysis tasks:

a. Missing Value Analysis

b. Outlier Treatment using the Z-score method

c. Deal with correlated variables

a. Missing Value Analysis
"""

# cheaking for missing values

df.isna().sum().sort_values(ascending=False)

# filling missing values

df.MINIMUM_PAYMENTS.fillna(0,inplace=True)
df.CREDIT_LIMIT.fillna(df.CREDIT_LIMIT.mode()[0],inplace=True)

# cheaking for missing values

df.isna().sum().sort_values(ascending=False)

# dropping cust_id

df=df.drop(['CUST_ID'],axis=1)

df.shape

# remove duplicates
df=df.drop_duplicates()

df.shape

"""b. Outlier Treatment using the Z-score method"""

df.info()

# getting columns

import numpy
col=df.columns.values

# IQR method (Inter Quantile Range Method)

p0=[]
p100=[]
iqr=[]

q1=[]
q2=[]
q3=[]


for i in col:

  if type(df[i][0]) in [numpy.float64,numpy.int64] :
    
    p0.append(df[i].min())

    p100.append(df[i].max())

    x=df[i].quantile(0.25)                   # 25th quantile

    y= df[i].quantile(0.50)                    # 50th quantile

    z= df[i].quantile(0.75)                    # 75th quantile
    
    q1.append(x)
    q2.append(y)
    q3.append(z)
    
    iqr.append(z-x)                            # Inter Quantile Range
  else:
    p0.append(None)

    p100.append(None)

    iqr.append(None)

    q1.append(None)
    q2.append(None)
    q3.append(None)

n=len(col)-1
for i in range(0,n):
  print(p0[i],'\t',p100[i],'\t',iqr[i])

lower_cutoff=[]
upper_cutoff=[]
i=0
for i in range(0,n):
  if (iqr[i]!=None) and (q1[i]!=None) and (q3[i]!=None):
    lower_cutoff.append(q1[i]- 1.5*iqr[i])
    upper_cutoff.append(q3[i]+ 1.5*iqr[i])
  else:
    lower_cutoff.append(None)
    upper_cutoff.append(None)

lower_cutoff

upper_cutoff

# clipping the outliers

# if the lower_cutoff < p0 -> There are no outliers on the lower side
# if the upper_cutoff > p100 -> There are no outliers on the higher side

i=0

for i in range(0,n):
    df[col[i]].clip(upper=upper_cutoff[i],inplace=True)
    df[col[i]].clip(lower=lower_cutoff[i],inplace=True)

# boxplot after cliping

for i in col:
  df[i].plot(title=i,kind='box')
  plt.show()

"""c. Deal with correlated variables"""

df.corr()

"""Q3. Perform dimensionality reduction using PCA such that the
95% of the variance is explained
"""

# PCA - Principal Compound Analysis
x=df.drop(['TENURE'],axis=1)
y=df['TENURE']
print(x)
print(y)

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y,train_size=0.60,test_size=0.40,random_state=47)

xtrain.shape,xtest.shape,ytrain.shape,ytest.shape

# classification

from sklearn.linear_model import LogisticRegression

model=LogisticRegression()

model.fit(xtrain,ytrain)

predicted_y=model.predict(xtest)

predicted_y

#to measure the accuracy of model
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
accuracy_score(ytest,predicted_y)

#making confusion matrix
confusion_matrix(ytest,predicted_y)

#classification report
z=classification_report(ytest,predicted_y)
print(z)

#PCA

from sklearn.decomposition import PCA

pca=PCA(n_components=0.95)

pca.fit(xtrain)

xtrain_transformed=pca.transform(xtrain)

print(xtrain_transformed.shape)

xtest_transformed=pca.transform(xtest)

print(xtest_transformed.shape)

new_model=LogisticRegression(penalty='l1',solver='liblinear')

new_model.fit(xtrain_transformed,ytrain)

new_predicition=new_model.predict(xtest_transformed)

ytest,predicted_y,new_predicition

# accuracy 

from sklearn.metrics import accuracy_score

new_acc=accuracy_score(new_predicition,ytest)

new_acc

"""Q4. Find the optimum value of k for k-means clustering using
the elbow method. Plot the elbow curve

"""

from sklearn.cluster import KMeans

wcss=[]  #With in cluster sum of squares

for i in range(1,8):
  kmeans=KMeans(n_clusters=i, 
                init='k-means++', 
                n_init=10, 
                max_iter=300, 
                tol=0.0001,  
                verbose=0, 
                random_state=None, 
                copy_x=True,  
                algorithm='auto')
  
  kmeans.fit(df)

  wcss.append(kmeans.inertia_)

import matplotlib.pyplot as plt

plt.plot(range(1,8),wcss)  #Elbow Graph

"""Q5. Find the optimum value of k for k-means clustering using
the silhouette score method and specify the number of
observations in each cluster using a bar plot
"""

import sklearn.metrics as metrics

for i in range(3,10):
    kmeans=KMeans(n_clusters=i,
                  init="k-means++",
                  random_state=200)
    kmeans.fit(df)
    labels=kmeans.labels_
    print ("Silhouette score for k(clusters) = "+str(i)+" is "
           +str(metrics.silhouette_score(df,
                                         labels,
                                         metric="euclidean",
                                         sample_size=1000,
                                         random_state=200)))
import matplotlib.pyplot as plt
from yellowbrick.cluster import SilhouetteVisualizer

fig, ax = plt.subplots(2, 2, figsize=(15,8))
for i in [2, 3, 4, 5]:
    '''
    Create KMeans instance for different number of clusters
    '''
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)
    q, mod = divmod(i, 2)
    '''
    Create SilhouetteVisualizer instance with KMeans instance
    Fit the visualizer
    '''
    visualizer = SilhouetteVisualizer(km, colors='RdGy', ax=ax[q-1][mod])
    visualizer.fit(x)

"""#Part II: Deep Learning """

# importing the dataset

d=pd.read_csv('/content/drive/MyDrive/Imarticus Datasets/Paper2/Sentiment.csv')

d

d.shape

d.info()

d=d.drop_duplicates()

d.shape

"""Q1. Print the total number of positive and negative sentiments."""

c1=0
c2=0
for i in d['sentiment']:
  if i=='Positive':
    c1=c1+1
  elif i=='Negative':
    c2=c2+1
print('Total no. of Positive sentiments :',c1)
print('Total no. of Negative sentiments :',c2)

"""Q2. Build a sequential LSTM model to predict positive and
negative sentiments.
"""



# Cleaning the texts
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

corpus=[]
for i in range(0, 13871):
    review = re.sub('[^a-zA-Z]', ' ', 
                    d['text'][i])
    review = review.lower()
    review = review.split()
    ps = PorterStemmer()
    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review)
    corpus.append(review)

corpus

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english', min_df=20)
x = vectorizer.fit_transform(d['text'])
y = d['sentiment']

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=0)

model = Sequential()

"""Q3.
Based on the model, check the sentiment for the following
two sentences

a. 'He is a great leader.'

b. 'He is a terrible leader.'

"""

s1='he is a great leader'
s2='he is a terrible leader'

